{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sClKYr9ZB7Pk"
   },
   "source": [
    "# Library/Comon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4101,
     "status": "ok",
     "timestamp": 1651389371757,
     "user": {
      "displayName": "Marat Kh",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "WV07IHkKds3_",
    "outputId": "697d089d-d500-4a04-cd72-9b7576df173b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dill in c:\\programdata\\anaconda3\\lib\\site-packages (0.3.4)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  ! pip install dill\n",
    "  import dill\n",
    "except:\n",
    "  import dill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.10.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1651389371777,
     "user": {
      "displayName": "Marat Kh",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "2CtRq6KhX6gT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import seaborn as sns\n",
    "import warnings; warnings.filterwarnings(action='once')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import datetime\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import random\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1651389371792,
     "user": {
      "displayName": "Marat Kh",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "3UBL2XDNX0JR"
   },
   "outputs": [],
   "source": [
    "# My data\n",
    "\n",
    "DIR = '/content/drive/MyDrive/Colab Notebooks/ROBO/My_Feature_from_NET/'\n",
    "\n",
    "# if torch.cuda.is_available():  \n",
    "#   device = \"cuda:0\" \n",
    "# else:  \n",
    "#   device = \"cpu\"\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # insted\n",
    "# DIR = ('F:/Мой диск/Colab Notebooks/ROBO/My_Feature_from_NET/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1651389371857,
     "user": {
      "displayName": "Marat Kh",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "0H7iAsAb9k18"
   },
   "outputs": [],
   "source": [
    "def profits(all):\n",
    "\n",
    "  all['profit_GEP'] = (all['Cl_firs_tomor']/all['Cl_last_today'] - 1)*100\n",
    "  all['profit_GEP_label'] = np.where(all['profit_GEP'] > 0, 1, 0)\n",
    "  \n",
    "  all['profit_1hour'] = np.where(all['Symbol'].shift(-1) == all['Symbol'].shift(0), (all['Close'].shift(-1)/all['Close'] - 1)*100, 0)\n",
    "\n",
    "  \n",
    "  return all, \n",
    "\n",
    "# add dificult profit\n",
    "\n",
    "def ad_profit_to_13(all, i, ii):\n",
    "  close_time = pd.pivot_table(all,\n",
    "                index = [all.DateTime.dt.date, all.Symbol],\n",
    "                columns = all.DateTime.dt.time,\n",
    "                values = 'Close'\n",
    "                ).reset_index()\n",
    "  close_time.sort_values(by = ['Symbol', 'DateTime'], inplace=True)\n",
    "  \n",
    "  close_time.dropna(inplace=True)\n",
    "  close_time.reset_index(inplace = True, drop = True)\n",
    "  close_time['DateTime'] = pd.to_datetime(close_time['DateTime'])\n",
    "  close_time['Date'] = close_time['DateTime'].dt.date\n",
    "  close_time['Date'] = pd.to_datetime(close_time['Date'])\n",
    "  close_time.columns = [str(i) for i in close_time.columns]\n",
    "\n",
    "  pr_name = 'profit_' + str(i) + '_to_' + str(ii)\n",
    "  close_time[pr_name] = np.where(close_time['Symbol'] == close_time['Symbol'].shift(-1), 100*(close_time[i].shift(-1)/close_time[ii] - 1), 0)\n",
    "  close_time[pr_name] = np.where(close_time[pr_name] > 20, 20, np.where(close_time[pr_name] < -20, -20, close_time[pr_name]))\n",
    "  pr_label = pr_name + '_label'\n",
    "  close_time[pr_label] = np.where(close_time[pr_name] > 0, 1, 0)\n",
    "  return pd.merge(close_time[['Date', 'Symbol', pr_name, pr_label]], all, on = ['Date', 'Symbol'], how = 'right'), pr_name, pr_label\n",
    "\n",
    "\n",
    "def lfl(df_f_all):\n",
    "  data_np = np.array(df_f_all['Date'].dt.date)\n",
    "  close_np = np.array(df_f_all['Close'])\n",
    "  # high_np = np.array(df_f_all['High'])\n",
    "  low_np = np.array(df_f_all['Low'])\n",
    "  # print(data_np.size == close_np.size == high_np.size == low_np.size)\n",
    "  k1 = []\n",
    "  Cl_last_yeast = np.zeros(shape=data_np.size)\n",
    "  Cl_last_today = np.zeros(shape=data_np.size)\n",
    "  Cl_firs_today = np.zeros(shape=data_np.size)\n",
    "  Cl_firs_tomor = np.zeros(shape=data_np.size)\n",
    "  Min_day = np.zeros(shape=data_np.size)\n",
    "  Max_day = np.zeros(shape=data_np.size)\n",
    "\n",
    "  for i in range(data_np.size):\n",
    "    if i == 0:\n",
    "      Cl_last_yeast[i] = close_np[i]\n",
    "    elif data_np[i] == data_np[i-1]:\n",
    "      Cl_last_yeast[i] = Cl_last_yeast[i-1]\n",
    "    else: \n",
    "      Cl_last_yeast[i] = close_np[i-1]\n",
    "\n",
    "  df_f_all['Cl_last_yeast'] = Cl_last_yeast\n",
    "\n",
    "  # #Short\n",
    "  k1 = []\n",
    "\n",
    "  for i in range(data_np.size-1, -1, -1):\n",
    "    # print(i)\n",
    "    if i == data_np.size - 1:\n",
    "      Cl_last_today[i]  =  close_np[i]\n",
    "      k = 1\n",
    "\n",
    "    elif data_np[i] == data_np[i+1]:\n",
    "\n",
    "      Cl_last_today[i]  =  Cl_last_today[i+1]\n",
    "      k += 1\n",
    "\n",
    "    else:\n",
    "\n",
    "      Cl_last_today[i]  = close_np[i]\n",
    "      k = 1\n",
    "    k1.append(k)\n",
    "\n",
    "  df_f_all['Short'] = k1[::-1]  \n",
    "  df_f_all['Cl_last_today'] = Cl_last_today\n",
    "\n",
    "  # Long\n",
    "  k1 = []\n",
    "\n",
    "  for i in range(data_np.size):\n",
    "    if i == 0:\n",
    "      Cl_firs_today[i]   =  close_np[i]\n",
    "      k = 1\n",
    "\n",
    "    elif data_np[i] == data_np[i-1]:\n",
    "\n",
    "      Cl_firs_today[i]   =  Cl_firs_today[i-1]\n",
    "      k += 1\n",
    "\n",
    "    else:\n",
    "\n",
    "      Cl_firs_today[i]  = close_np[i]\n",
    "      k = 1\n",
    "    k1.append(k)\n",
    "\n",
    "  df_f_all['Long'] = k1\n",
    "  df_f_all['Cl_firs_today'] = Cl_firs_today \n",
    "\n",
    "  # Clo_tomor\n",
    "\n",
    "  for i in range(data_np.size-1, -1, -1):\n",
    "\n",
    "    if i == data_np.size - 1:\n",
    "      Cl_firs_tomor[i]  =  close_np[i]\n",
    "\n",
    "    elif data_np[i] == data_np[i+1]:\n",
    "      Cl_firs_tomor[i]  =  Cl_firs_tomor[i+1]\n",
    "\n",
    "    else:\n",
    "      Cl_firs_tomor[i]  = close_np[i+1]\n",
    "  \n",
    "  df_f_all['Cl_firs_tomor'] = Cl_firs_tomor\n",
    "\n",
    "\n",
    "  # MinD\n",
    "\n",
    "  for i in range(data_np.size):\n",
    "    if i == 0:\n",
    "      Min_day[i] = low_np[i]\n",
    "\n",
    "    elif data_np[i] == data_np[i-1]:\n",
    "      if low_np[i] < Min_day[i-1]:\n",
    "        Min_day[i] = low_np[i]\n",
    "      else:\n",
    "        Min_day[i] = Min_day[i-1]\n",
    "\n",
    "    else: \n",
    "      Min_day[i] = low_np[i]\n",
    "\n",
    "  df_f_all['Min_day'] = Min_day\n",
    "\n",
    "\n",
    "\n",
    "  # MaxD\n",
    "\n",
    "  # for i in range(data_np.size):\n",
    "  #   if i == 0:\n",
    "  #     Max_day[i] = high_np[i]\n",
    "\n",
    "  #   elif data_np[i] == data_np[i-1]:\n",
    "  #     if high_np[i] > Max_day[i-1]:\n",
    "  #       Max_day[i] = high_np[i]\n",
    "  #     else:\n",
    "  #       Max_day[i] = Max_day[i-1]\n",
    "\n",
    "  #   else: \n",
    "  #     Max_day[i] = high_np[i]\n",
    "\n",
    "  # df_f_all['Max_day'] = Max_day\n",
    "\n",
    "def use_lfl(df_f_all):\n",
    "  df_all = pd.DataFrame()\n",
    "  for i in df_f_all['Symbol'].unique():\n",
    "    df = df_f_all[df_f_all['Symbol'] == i].copy()\n",
    "    lfl(df)\n",
    "    df_all = pd.concat([df_all, df])\n",
    "    df_all.reset_index(inplace=True, drop = True)\n",
    "  return df_all\n",
    "\n",
    "\n",
    "def hide(df):\n",
    "  df['Cl_min_day']      = (df['Close']/df['Min_day']-1)*100\n",
    "\n",
    "  df['Proft1000']       = (df['Cl_firs_tomor']/df['Close'].shift(0)-1)*100\n",
    "  \n",
    "  return df\n",
    "\n",
    "# def data(all):\n",
    "#   all['Date'] = pd.to_datetime(all['Date'])\n",
    "#   all['DateTime'] = all['Date']\n",
    "#   all['DateTime'] = pd.to_datetime(all['DateTime'])\n",
    "#   all['Date'] = all['DateTime'].dt.date\n",
    "#   all['Date'] = pd.to_datetime(all['Date'])\n",
    "#   # all = all[(all['DateTime'].dt.time >= datetime.time(10,10))&(all['DateTime'].dt.time <= datetime.time(18,30))]\n",
    "#   # all = all[(all['DateTime'].dt.year >=2011)]\n",
    "#   return all  \n",
    "\n",
    "def generate(data_, range_, cols = ['Cl_min_day']):\n",
    "  features = []\n",
    "  for col in cols:\n",
    "    empty = np.empty(shape = (data_.shape[0], range_))\n",
    "    for sym in tqdm(data_['Symbol'].unique()):\n",
    "      np_col = 0\n",
    "      df_sym = data_[data_['Symbol'] == sym]  \n",
    "      for i in (range(range_)):\n",
    "        empty[df_sym.index[0]:df_sym.index[-1] + 1, np_col] = df_sym[col].shift(i)\n",
    "        np_col +=1\n",
    "    feature = [str(col) + '_' + str(i) for i in range(range_)]\n",
    "    features.append(feature)\n",
    "    data_[feature] = empty\n",
    "  \n",
    "  return data_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZQySXn9CDG9"
   },
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 846,
     "status": "ok",
     "timestamp": 1651388959435,
     "user": {
      "displayName": "Marat Kh",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "XyPwzZAec76e",
    "outputId": "a853b875-48a7-494b-cb60-005442eda338"
   },
   "outputs": [],
   "source": [
    "file = 'all_hour1800_300422.csv'\n",
    "# all_ = pd.read_csv(DIR + 'Data/all/' + file, index_col=[0])\n",
    "all_ = pd.read_csv(file, index_col=[0])\n",
    "\n",
    "# all[['Date',\t'Symbol', 'Open',\t'High',\t'Low',\t'Close',\t'Volume']].to_csv(DIR + 'Data/all/' + 'all_hour1800_300422.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1651388967174,
     "user": {
      "displayName": "Marat Kh",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "JXms0NJoLkre"
   },
   "outputs": [],
   "source": [
    "all = all_.iloc[-100000:, :].reset_index(drop = True)\n",
    "all.drop(columns = ['Open', 'High', 'Volume'], inplace = True)\n",
    "\n",
    "y_pers = np.where(all['Symbol'].shift(-1) == all['Symbol'].shift(0), (all['Close'].shift(-1)/all['Close'] - 1)*100, 0)\n",
    "y = np.where(y_pers  > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1651388969098,
     "user": {
      "displayName": "Marat Kh",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "p3yochNFLkt9",
    "outputId": "df13ad66-fad6-43f0-9ed8-96514091b372"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75000, 4) (75000,) (25000, 4) (25000,)\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(all, y)\n",
    "X_train = all[:int(len(all)*0.75)]\n",
    "X_test = all[int(len(all)*0.75):]\n",
    "X_test = X_test.reset_index(drop = True)\n",
    "\n",
    "y_train = y[:int(len(all)*0.75)]\n",
    "y_test = y[int(len(all)*0.75):]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ws4_RTaIclO"
   },
   "source": [
    "## Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1651389820605,
     "user": {
      "displayName": "Marat Kh",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "PbjkUBE5gOrA",
    "outputId": "ac8148b1-e204-4f0b-8c9f-6ccb731608fe"
   },
   "outputs": [],
   "source": [
    "# [str(col) + '_' + str(i) for i in range(range_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UEaVEyZaiMZW"
   },
   "outputs": [],
   "source": [
    "# use_lfl(data(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1651390410944,
     "user": {
      "displayName": "Marat Kh",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "jGYoMWi9fera"
   },
   "outputs": [],
   "source": [
    "def generate(data_, range_, cols = ['Cl_min_day']):\n",
    "  features = []\n",
    "  for col in cols:\n",
    "    empty = np.empty(shape = (data_.shape[0], range_))\n",
    "    for sym in tqdm(data_['Symbol'].unique()):\n",
    "      np_col = 0\n",
    "      df_sym = data_[data_['Symbol'] == sym]  \n",
    "      for i in (range(range_)):\n",
    "        empty[df_sym.index[0]:df_sym.index[-1] + 1, np_col] = df_sym[col].shift(i)\n",
    "        np_col +=1\n",
    "    feature = [str(col) + '_' + str(i) for i in range(range_)]\n",
    "    # features.append(feature)\n",
    "    # data_[feature] = empty\n",
    "    # data_ = data_.join(pd.DataFrame(empty, columns=feature))\n",
    "  \n",
    "  return data_.join(pd.DataFrame(empty, columns=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1651391933588,
     "user": {
      "displayName": "Marat Kh",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "jCIeC6fTxkdN"
   },
   "outputs": [],
   "source": [
    "class ExperimentalTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "  def fit(self, X, y = None):\n",
    "\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "\n",
    "    X_ = X.copy() \n",
    "    print(X_)\n",
    "    X_ = self.data(X_)\n",
    "    X_ = self.use_lfl(X_)\n",
    "    X_ = self.hide(X_)\n",
    "    X_ = self.generate(X_, 9, ['Cl_min_day'])[['Cl_min_day_' + str(i) for i in range(9)]].fillna(0)\n",
    "\n",
    "    return X_\n",
    "  def generate(self, data_, range_, cols = ['Cl_min_day']):\n",
    "      features = []\n",
    "      for col in cols:\n",
    "        empty = np.empty(shape = (data_.shape[0], range_))\n",
    "        for sym in tqdm(data_['Symbol'].unique()):\n",
    "          np_col = 0\n",
    "          df_sym = data_[data_['Symbol'] == sym]  \n",
    "          for i in (range(range_)):\n",
    "            empty[df_sym.index[0]:df_sym.index[-1] + 1, np_col] = df_sym[col].shift(i)\n",
    "            np_col +=1\n",
    "        feature = [str(col) + '_' + str(i) for i in range(range_)]\n",
    "        # features.append(feature)\n",
    "        # data_[feature] = empty\n",
    "        # data_ = data_.join(pd.DataFrame(empty, columns=feature))\n",
    "\n",
    "      return data_.join(pd.DataFrame(empty, columns=feature))\n",
    "  def hide(self, df):\n",
    "      df['Cl_min_day']      = (df['Close']/df['Min_day']-1)*100\n",
    "\n",
    "      df['Proft1000']       = (df['Cl_firs_tomor']/df['Close'].shift(0)-1)*100\n",
    "\n",
    "      return df\n",
    "\n",
    "  def data(self, all):\n",
    "      all['Date'] = pd.to_datetime(all['Date'])\n",
    "      all['DateTime'] = all['Date']\n",
    "      all['DateTime'] = pd.to_datetime(all['DateTime'])\n",
    "      all['Date'] = all['DateTime'].dt.date\n",
    "      all['Date'] = pd.to_datetime(all['Date'])\n",
    "      # all = all[(all['DateTime'].dt.time >= datetime.time(10,10))&(all['DateTime'].dt.time <= datetime.time(18,30))]\n",
    "      # all = all[(all['DateTime'].dt.year >=2011)]\n",
    "      return all  \n",
    "  def use_lfl(self, df_f_all):\n",
    "      df_all = pd.DataFrame()\n",
    "      for i in df_f_all['Symbol'].unique():\n",
    "        df = df_f_all[df_f_all['Symbol'] == i].copy()\n",
    "        self.lfl(df)\n",
    "        df_all = pd.concat([df_all, df])\n",
    "        df_all.reset_index(inplace=True, drop = True)\n",
    "      return df_all\n",
    "\n",
    "  def lfl(self, df_f_all):\n",
    "      data_np = np.array(df_f_all['Date'].dt.date)\n",
    "      close_np = np.array(df_f_all['Close'])\n",
    "      # high_np = np.array(df_f_all['High'])\n",
    "      low_np = np.array(df_f_all['Low'])\n",
    "      # print(data_np.size == close_np.size == high_np.size == low_np.size)\n",
    "      k1 = []\n",
    "      Cl_last_yeast = np.zeros(shape=data_np.size)\n",
    "      Cl_last_today = np.zeros(shape=data_np.size)\n",
    "      Cl_firs_today = np.zeros(shape=data_np.size)\n",
    "      Cl_firs_tomor = np.zeros(shape=data_np.size)\n",
    "      Min_day = np.zeros(shape=data_np.size)\n",
    "      Max_day = np.zeros(shape=data_np.size)\n",
    "\n",
    "      for i in range(data_np.size):\n",
    "        if i == 0:\n",
    "          Cl_last_yeast[i] = close_np[i]\n",
    "        elif data_np[i] == data_np[i-1]:\n",
    "          Cl_last_yeast[i] = Cl_last_yeast[i-1]\n",
    "        else: \n",
    "          Cl_last_yeast[i] = close_np[i-1]\n",
    "\n",
    "      df_f_all['Cl_last_yeast'] = Cl_last_yeast\n",
    "\n",
    "      # #Short\n",
    "      k1 = []\n",
    "\n",
    "      for i in range(data_np.size-1, -1, -1):\n",
    "        # print(i)\n",
    "        if i == data_np.size - 1:\n",
    "          Cl_last_today[i]  =  close_np[i]\n",
    "          k = 1\n",
    "\n",
    "        elif data_np[i] == data_np[i+1]:\n",
    "\n",
    "          Cl_last_today[i]  =  Cl_last_today[i+1]\n",
    "          k += 1\n",
    "\n",
    "        else:\n",
    "\n",
    "          Cl_last_today[i]  = close_np[i]\n",
    "          k = 1\n",
    "        k1.append(k)\n",
    "\n",
    "      df_f_all['Short'] = k1[::-1]  \n",
    "      df_f_all['Cl_last_today'] = Cl_last_today\n",
    "\n",
    "      # Long\n",
    "      k1 = []\n",
    "\n",
    "      for i in range(data_np.size):\n",
    "        if i == 0:\n",
    "          Cl_firs_today[i]   =  close_np[i]\n",
    "          k = 1\n",
    "\n",
    "        elif data_np[i] == data_np[i-1]:\n",
    "\n",
    "          Cl_firs_today[i]   =  Cl_firs_today[i-1]\n",
    "          k += 1\n",
    "\n",
    "        else:\n",
    "\n",
    "          Cl_firs_today[i]  = close_np[i]\n",
    "          k = 1\n",
    "        k1.append(k)\n",
    "\n",
    "      df_f_all['Long'] = k1\n",
    "      df_f_all['Cl_firs_today'] = Cl_firs_today \n",
    "\n",
    "      # Clo_tomor\n",
    "\n",
    "      for i in range(data_np.size-1, -1, -1):\n",
    "\n",
    "        if i == data_np.size - 1:\n",
    "          Cl_firs_tomor[i]  =  close_np[i]\n",
    "\n",
    "        elif data_np[i] == data_np[i+1]:\n",
    "          Cl_firs_tomor[i]  =  Cl_firs_tomor[i+1]\n",
    "\n",
    "        else:\n",
    "          Cl_firs_tomor[i]  = close_np[i+1]\n",
    "\n",
    "      df_f_all['Cl_firs_tomor'] = Cl_firs_tomor\n",
    "\n",
    "\n",
    "      # MinD\n",
    "\n",
    "      for i in range(data_np.size):\n",
    "        if i == 0:\n",
    "          Min_day[i] = low_np[i]\n",
    "\n",
    "        elif data_np[i] == data_np[i-1]:\n",
    "          if low_np[i] < Min_day[i-1]:\n",
    "            Min_day[i] = low_np[i]\n",
    "          else:\n",
    "            Min_day[i] = Min_day[i-1]\n",
    "\n",
    "        else: \n",
    "          Min_day[i] = low_np[i]\n",
    "\n",
    "        df_f_all['Min_day'] = Min_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "UPlx8V69yZWZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create pipeline 2\n",
      "fit pipeline 2\n",
      "                      Date Symbol        Low      Close\n",
      "0      2017-08-09 16:00:00   TATN  391.70000  392.50000\n",
      "1      2017-08-09 17:00:00   TATN  388.30000  390.65000\n",
      "2      2017-08-10 10:00:00   TATN  391.35000  393.10000\n",
      "3      2017-08-10 11:00:00   TATN  390.75000  391.60000\n",
      "4      2017-08-10 12:00:00   TATN  391.00000  391.90000\n",
      "...                    ...    ...        ...        ...\n",
      "74995  2016-11-14 13:00:00   VTBR    0.06858    0.06866\n",
      "74996  2016-11-14 14:00:00   VTBR    0.06840    0.06935\n",
      "74997  2016-11-14 15:00:00   VTBR    0.06871    0.06877\n",
      "74998  2016-11-14 16:00:00   VTBR    0.06850    0.06852\n",
      "74999  2016-11-14 17:00:00   VTBR    0.06831    0.06836\n",
      "\n",
      "[75000 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 143.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('transform_data', ExperimentalTransformer()),\n",
       "                ('linear_model', LinearRegression())])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with input transformation\n",
    "print(\"create pipeline 2\")\n",
    "pipe2 = Pipeline(steps=[\n",
    "                       ('transform_data', ExperimentalTransformer()),    # this will trigger a call to __init__\n",
    "                       ('linear_model', LinearRegression())\n",
    "                      #  ('GB', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "print(\"fit pipeline 2\")\n",
    "pipe2.fit(X_train, y_train)\n",
    "\n",
    "# preds2 = pipe2.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1651390486628,
     "user": {
      "displayName": "Marat Kh",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "tA13LDhqTA_U"
   },
   "outputs": [],
   "source": [
    "with open(\"D:/logreg_pipeline3.dill\", \"wb\") as f:\n",
    "    dill.dump(pipe2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1651396859999,
     "user": {
      "displayName": "Marat Kh",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "gv71DitESZFQ"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json      \n",
    "\n",
    "\n",
    "def get_prediction(x):\n",
    "    Date, Symbol, Low, Close = x\n",
    "    body = {'Date': Date, \n",
    "            'Symbol': Symbol,\n",
    "            'Low': Low,\n",
    "            'Close': Close\n",
    "            } \n",
    "\n",
    "    myurl = \"http://127.0.0.1:8180/predict\"\n",
    "    # myurl = \"http://127.0.0.1:8180\"\n",
    "    req = urllib.request.Request(myurl)\n",
    "    req.add_header('Content-Type', 'application/json; charset=utf-8')\n",
    "    jsondata = json.dumps(body)\n",
    "    print(jsondata)\n",
    "    jsondataasbytes = jsondata.encode('utf-8')   # needs to be bytes\n",
    "    req.add_header('Content-Length', len(jsondataasbytes))\n",
    "    #print (jsondataasbytes)\n",
    "    response = urllib.request.urlopen(req, jsondataasbytes)\n",
    "    return json.loads(response.read())['predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# добавил запрос"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "ar = requests.post('http://127.0.0.1:8180/predict', json=X_test[:10].to_json())\n",
    "ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4699121037,\n",
       " 0.470785284,\n",
       " 0.4738658043,\n",
       " 0.4709035918,\n",
       " 0.4761687491,\n",
       " 0.4786982541,\n",
       " 0.4758647877,\n",
       " 0.4711836916,\n",
       " 0.4771656822,\n",
       " 0.4771497252]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(ar.json())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('http://127.0.0.1:8180/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# дальше не читал) обрати внимание - ниже код отправки запроса изз твоей функции отправляет только название колонок(добавил print в функцию - пытается отправить лишь \n",
    "{\"Date\": \"Date\", \"Symbol\": \"Symbol\", \"Low\": \"Low\", \"Close\": \"Close\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1651396860314,
     "user": {
      "displayName": "Marat Kh",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "GKJmiTO6UVpY",
    "outputId": "da1b1cd7-2bea-4a57-89db-deb7a9c28871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Date\": \"Date\", \"Symbol\": \"Symbol\", \"Low\": \"Low\", \"Close\": \"Close\"}\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 500: INTERNAL SERVER ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12364/2413468282.py\u001b[0m in \u001b[0;36mget_prediction\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Content-Length'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjsondataasbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m#print (jsondataasbytes)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjsondataasbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'predictions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m             response = self.parent.error(\n\u001b[0m\u001b[0;32m    633\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 500: INTERNAL SERVER ERROR"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = get_prediction(X_test[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1651396865065,
     "user": {
      "displayName": "Marat Kh",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "Xlr8LK_WdFfD",
    "outputId": "861fea29-c6b9-470e-8795-ddcbbd07551e"
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TF8T3CGnA5AD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX8yY9mEA-h9"
   },
   "source": [
    "# Павел. Дальше можно не читать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAtUuxQoA5DD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgzmfABZA5Fr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFzT4rhaA5I3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-etkfh_5JeP"
   },
   "source": [
    "# GENERATE TEST_TRAIN SET FOR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1317,
     "status": "ok",
     "timestamp": 1651342257722,
     "user": {
      "displayName": "Marat Kh",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "6_yMJELV5TVk",
    "outputId": "3fd5479f-9928-41dc-d91e-e730a5a9dc7b"
   },
   "outputs": [],
   "source": [
    "def df_data_def(all, features, bars_in_day, label_feature_day, label = 'Short', label_mark = 1):\n",
    "  # features = ['Min10', 'Min1000']\n",
    "  # features = ['Cl_min_day']\n",
    "  target = 'Proft1000' # тут могут быть несколько target - по нему обучаемся, target1 - доп параметр по которому не учимся\n",
    "  feature_name = []\n",
    "  name_profit_label = str(target) + '_label'\n",
    "  name_profit = target\n",
    "  \n",
    "  df_data = pd.DataFrame({'Date': all[all[label] == label_mark]['DateTime'].dt.date, 'Symbol': all[all[label] == label_mark]['Symbol'], target: all[all[label] == label_mark][target]}).drop_duplicates()\n",
    "  df_data['Date'] = pd.to_datetime(df_data['Date'])\n",
    "  for feature in features:\n",
    "\n",
    "    new_date = pd.pivot_table(all,\n",
    "                  index = [all.DateTime.dt.date, all.Symbol],\n",
    "                  columns = all.DateTime.dt.time,\n",
    "                  values = feature\n",
    "                  ).iloc[:,:bars_in_day].reset_index()\n",
    "\n",
    "\n",
    "  df_data[target] = np.where(df_data[target] > 20, 20, np.where(df_data[target] < -20, -20, df_data[target]))\n",
    "  df_data[str(target) + '_label'] = np.where(df_data[target] > 0, 1, 0)\n",
    "  df_data.dropna(inplace=True)\n",
    "  df_data.sort_values(by = ['Date', 'Symbol'], inplace=True, ignore_index=True)\n",
    "  display(df_data.shape)\n",
    "  return df_data, feature_name, name_profit_label, name_profit, target, features\n",
    "\n",
    "label_mark = 1\n",
    "bars_in_day = 9\n",
    "label = 'Short'\n",
    "features = ['Cl_min_day']\n",
    "df_data, feature_name, name_profit_label, name_profit, target, features  = df_data_def(all, bars_in_day=bars_in_day, label = label, label_mark = label_mark, label_feature_day = 'today', features = features)\n",
    "\n",
    "df_data_conect = all[all[label] == label_mark].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 811
    },
    "executionInfo": {
     "elapsed": 721,
     "status": "ok",
     "timestamp": 1651342437686,
     "user": {
      "displayName": "Marat Kh",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "uNgOhSRMo5Mc",
    "outputId": "5fed4ee6-640f-4e61-a77e-5dc86bdcdaf2"
   },
   "outputs": [],
   "source": [
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsABeGDmX9g7"
   },
   "outputs": [],
   "source": [
    "# df_data.to_excel(DIR + 'delete.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isUWvqGY_HRe"
   },
   "source": [
    "# Подгонка\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1642421851162,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "Kk_h7XqI7k3f",
    "outputId": "dd9ce539-587c-4075-9a89-6788c374c9df"
   },
   "outputs": [],
   "source": [
    "# feature_multy, x_train, y_train, x_test, y_test  = concat_of_split_1year(df_data_blue, year_from = 2019, axis = 2, lag = 3)\n",
    "feature_multy, x_train, y_train, x_test, y_test  = reshape_concat_of_split_1year(df_data, feature_name=feature_name, year_from = 2019)\n",
    "\n",
    "print(f'Check: {feature_multy.shape, x_train.shape, x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1642421851163,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "vGydgF3Y_OzE",
    "outputId": "6800b65b-f01e-4b49-bb7f-7d1b5d1a9dc6"
   },
   "outputs": [],
   "source": [
    "model_param = str('_'.join(features))\n",
    "shape_set = str(x_train.shape[-2]) + '_' + str(x_train.shape[-1])\n",
    "proba_1 = model_param + '_' + shape_set\n",
    "print(str('_'.join(features)), shape_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Maqi9XBxQa7"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaCC_1jY5XqT"
   },
   "source": [
    "### NeMod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135
    },
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1641883772364,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "2lyoOqMD5aFK",
    "outputId": "282b1413-1fe9-4c12-9470-f5d4b8b39e0f"
   },
   "outputs": [],
   "source": [
    "feature_multy, x_train, y_train, x_test, y_test  = concat_of_split_1year(df_data, year_from = 2014, axis = 2, lag = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5aquefQ0qQi"
   },
   "source": [
    "### GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1sAeaoq0uKk"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# fit model no training data\n",
    "model = XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lY9HX7fg0uXq"
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 226102,
     "status": "ok",
     "timestamp": 1641715124997,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "QCjAkYP_1L9q",
    "outputId": "98bccd76-7153-4e1e-f3e7-e36c6a38406b"
   },
   "outputs": [],
   "source": [
    "# надо ли перемешивать на тесте чтоы при батче 64 мы дивгали веса участь на всех бумажах а не на одной\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "matrix_all = pd.DataFrame()\n",
    "# epochs = 100\n",
    "# patience = 10\n",
    "start_year = 2011\n",
    "end_year = 2021 + 1 \n",
    "\n",
    "\n",
    "for year_from in range(start_year, end_year, 1):  \n",
    "  lag = 'all'\n",
    "  feature_multy, x_train, y_train, x_test, y_test  = concat_of_split_1year(df_data, year_from = year_from, axis = 2, lag = lag)\n",
    "  x_train = x_train.reshape(-1, 9*4)\n",
    "  x_test = x_test.reshape(-1, 9*4)\n",
    "  input_shape = x_train.shape[1:]\n",
    "    \n",
    "  # checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks/ROBO/My_Feature_from_NET/keras/' + dir_model + model_param + '_' + shape_set + '/' + str(year_from)\n",
    "  checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks/ROBO/My_Feature_from_NET/XGB/keras_from10_1830to60min/40/Gep/'\\\n",
    "                         + model_param + '_' + shape_set + '/'  + str(year_from)\n",
    "  print(checkpoint_filepath)\n",
    "  print(f'Test year: {year_from}')\n",
    "########################\n",
    "\n",
    "  model.fit(\n",
    "      x_train,\n",
    "      y_train,\n",
    "      )\n",
    "\n",
    "  # matrix_year = df_data[(df_data['Date'].dt.year == year_from)].copy()\n",
    "  # matrix_year[proba_1] = model.predict_proba(x_test)[:,-1]\n",
    "  # matrix_all = matrix_all.append(matrix_year)\n",
    "  # print(matrix_all.shape, matrix_year.shape)\n",
    "\n",
    "  # save\n",
    "  try:\n",
    "      os.stat(checkpoint_filepath)\n",
    "  except:\n",
    "      os.makedirs(checkpoint_filepath)\n",
    "  joblib.dump(model, checkpoint_filepath + '/CLF.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2ZmHqhG-xbU"
   },
   "source": [
    "### RandomFores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1641713350635,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "2minL75v-5V3",
    "outputId": "9eb7d1b6-64c6-4176-befb-c15524b4059d"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDptmR3W_NcA"
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=20, random_state=42, max_depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbCjC3EI-17V"
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 50529,
     "status": "ok",
     "timestamp": 1641714344975,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "yXoiGUWg__ej",
    "outputId": "51324f4c-23d4-47ee-8229-da3f22e71b03"
   },
   "outputs": [],
   "source": [
    "# надо ли перемешивать на тесте чтоы при батче 64 мы дивгали веса участь на всех бумажах а не на одной\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "matrix_all = pd.DataFrame()\n",
    "# epochs = 100\n",
    "# patience = 10\n",
    "start_year = 2011\n",
    "end_year = 2021 + 1 \n",
    "\n",
    "\n",
    "for year_from in range(start_year, end_year, 1):  \n",
    "  lag = 'all'\n",
    "  feature_multy, x_train, y_train, x_test, y_test  = concat_of_split_1year(df_data, year_from = year_from, axis = 2, lag = lag)\n",
    "  x_train = x_train.reshape(-1, 9*4)\n",
    "  x_test = x_test.reshape(-1, 9*4)\n",
    "  input_shape = x_train.shape[1:]\n",
    "    \n",
    "  # checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks/ROBO/My_Feature_from_NET/keras/' + dir_model + model_param + '_' + shape_set + '/' + str(year_from)\n",
    "  checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks/ROBO/My_Feature_from_NET/RFC_20est/keras_from10_1830to60min/40/Gep/'\\\n",
    "                         + model_param + '_' + shape_set + '/'  + str(year_from)\n",
    "  print(checkpoint_filepath)\n",
    "  print(f'Test year: {year_from}')\n",
    "########################\n",
    "\n",
    "  model.fit(\n",
    "      x_train,\n",
    "      y_train,\n",
    "      )\n",
    "\n",
    "  # matrix_year = df_data[(df_data['Date'].dt.year == year_from)].copy()\n",
    "  # matrix_year[proba_1] = model.predict_proba(x_test)[:,-1]\n",
    "  # matrix_all = matrix_all.append(matrix_year)\n",
    "  # print(matrix_all.shape, matrix_year.shape)\n",
    "\n",
    "  # save\n",
    "  try:\n",
    "      os.stat(checkpoint_filepath)\n",
    "  except:\n",
    "      os.makedirs(checkpoint_filepath)\n",
    "  joblib.dump(model, checkpoint_filepath + '/CLF.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRgnFo2xxTay"
   },
   "source": [
    "## Keras LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IosPzzVNyOQi"
   },
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtI2Qo1_XVVg"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1814,
     "status": "ok",
     "timestamp": 1641884947886,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "SuoDgSyeXSwR",
    "outputId": "3e713c07-94f1-4b98-c71d-3ccde5d4a91d"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units = 1))\n",
    "\n",
    "# out = model(x_train)\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qof6swzMxhuN"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6297687,
     "status": "ok",
     "timestamp": 1641631173487,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "G-N9qVACxWj8",
    "outputId": "d538895f-85e3-416f-b15d-287b748cefb1"
   },
   "outputs": [],
   "source": [
    "# надо ли перемешивать на тесте чтоы при батче 64 мы дивгали веса участь на всех бумажах а не на одной\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "matrix_all = pd.DataFrame()\n",
    "epochs = 100\n",
    "patience = 10\n",
    "start_year = 2011\n",
    "end_year = 2021 + 1 \n",
    "\n",
    "\n",
    "for year_from in range(start_year, end_year, 1):  \n",
    "  lag = 'all'\n",
    "  feature_multy, x_train, y_train, x_test, y_test  = concat_of_split_1year(df_data, year_from = year_from, axis = 2, lag = lag)\n",
    "  input_shape = x_train.shape[1:]\n",
    "    \n",
    "  # checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks/ROBO/My_Feature_from_NET/keras/' + dir_model + model_param + '_' + shape_set + '/' + str(year_from)\n",
    "  checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks/ROBO/My_Feature_from_NET/keras_lstm/keras_from10_1830to60min/40/Gep/'\\\n",
    "                         + model_param + '_' + shape_set + '/'  + str(year_from)\n",
    "  print(checkpoint_filepath)\n",
    "  print(f'Test year: {year_from}')\n",
    "########################\n",
    "\n",
    "  checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, \n",
    "                              monitor='val_accuracy',\n",
    "                              verbose=1, \n",
    "                              save_best_only=True,\n",
    "                              mode='auto')\n",
    "  EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience = patience, restore_best_weights=True)\n",
    "  # model = build_model(\n",
    "  #     input_shape,\n",
    "  #     head_size=40,\n",
    "  #     num_heads=8,\n",
    "  #     ff_dim=4,\n",
    "  #     num_transformer_blocks=4,\n",
    "  #     mlp_units=[128],\n",
    "  #     mlp_dropout=0.4,\n",
    "  #     dropout=0.25,\n",
    "  # )\n",
    "\n",
    "  model.compile(\n",
    "      loss=\"binary_crossentropy\",\n",
    "      optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "      metrics=['accuracy'],\n",
    "  )\n",
    "  # model.summary()\n",
    "\n",
    "  callbacks = [checkpoint, EarlyStopping]\n",
    "\n",
    "  model.fit(\n",
    "      x_train,\n",
    "      y_train,\n",
    "      validation_split=0.2,\n",
    "      epochs=epochs,\n",
    "      batch_size=64,\n",
    "      callbacks=callbacks)\n",
    "\n",
    "  model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "  matrix_year = df_data[(df_data['Date'].dt.year == year_from)].copy()\n",
    "  matrix_year[proba_1] = model.predict(x_test)[:,-1]\n",
    "  matrix_all = matrix_all.append(matrix_year)\n",
    "  print(matrix_all.shape, matrix_year.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiG3NLvAhIsn"
   },
   "source": [
    "## Pytorch LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqfjXNdKQP4i"
   },
   "outputs": [],
   "source": [
    "class Dataset_my(torch.utils.data.Dataset):\n",
    "  def __init__(self, x, y):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return {'sample': self.x[idx,:, :], 'target': self.y[idx]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlrlZVJoAQde"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DJ264j0F0re"
   },
   "outputs": [],
   "source": [
    "class My_LSTM(nn.Module):\n",
    "  def __init__(self, input_size, seq_length, hidden_size, num_layers, num_classes):\n",
    "    super(My_LSTM, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.seq_length = seq_length\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    self.num_classes = num_classes\n",
    "    self.lstm = nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_size, num_layers = self.num_layers, batch_first = True)\n",
    "    self.linear = nn.Linear(self.hidden_size, self.num_classes)\n",
    "    self.soft = nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # input.shape -> (batch_size, seq, input_size)\n",
    "    # h_0.shape -> (num_layer, batch, hidden_size)\n",
    "    # h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "    # c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "    # Initializing hidden state for first input with zeros\n",
    "    h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "    # Initializing cell state for first input with zeros\n",
    "    c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "\n",
    "    out, _ = self.lstm(x, (h_0, c_0))\n",
    "    out = out[:, -1, :]    \n",
    "    out = self.linear(out)\n",
    "    out = self.soft(out)\n",
    "    return out\n",
    "    \n",
    "    # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGwvY2okH6US"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# input_size = 784 # 28x28\n",
    "num_classes = 2\n",
    "num_epochs = 30\n",
    "batch_size = 128\n",
    "learning_rate = 0.1\n",
    "\n",
    "input_size = 4\n",
    "sequence_length = 9\n",
    "hidden_size = 50\n",
    "num_layers = 3\n",
    "\n",
    "model = My_LSTM(input_size=input_size, seq_length=sequence_length, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xoF2hbJF9-X1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1097,
     "status": "ok",
     "timestamp": 1641885871050,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "CWFEpo7J8f_H",
    "outputId": "2a4edad1-d06b-4c96-94de-fdb5348690d5"
   },
   "outputs": [],
   "source": [
    "outP = model(torch.tensor(x_train).float())\n",
    "outP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdI9_fBb-Ntn"
   },
   "outputs": [],
   "source": [
    "outP = outP.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1641885076953,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "m1e0LQC090j_",
    "outputId": "f8d9f7f9-6f38-4275-b6da-c3307e814da4"
   },
   "outputs": [],
   "source": [
    "outP - out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEuocC8CAUjf"
   },
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3dpLHs0Yact"
   },
   "outputs": [],
   "source": [
    "class CNN_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(nn.Conv1d(9, 4, kernel_size=2),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(nn.Conv1d(4, 32, kernel_size=3),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool1d(kernel_size=3, stride=2))\n",
    "        \n",
    "        # Расчет входных признаков по формуле Lout=((Lin+2*pading - dilation*(kernel - 1) - 1)/stride) + 1\n",
    "        self.classifier = nn.Sequential(nn.Linear(in_features=906*32, out_features=512, bias=True),\n",
    "                                        nn.ReLU(True),\n",
    "                                        nn.Dropout(),\n",
    "                                        nn.Linear(in_features=512, out_features=512, bias=True),\n",
    "                                        nn.ReLU(True),\n",
    "                                        nn.Dropout(),\n",
    "                                        nn.Linear(512, 2))\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x) \n",
    "        # out = self.layer2(out)\n",
    "        print(out.shape) \n",
    "        out = out.reshape(out.size(0), -1) \n",
    "        out = self.classifier(out) \n",
    "        return out\n",
    "\n",
    "model_CNN = CNN_classifier()\n",
    "# model = model.float()\n",
    "\n",
    "# Параметры обучения\n",
    "num_epochs = 3\n",
    "num_classes = 2\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_CNN.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "error",
     "timestamp": 1640342458490,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "QO9WyMqfA7M2",
    "outputId": "502c4b90-355a-41fd-e965-022a034ad67e"
   },
   "outputs": [],
   "source": [
    "model_CNN(set_['sample'].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1640342236138,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "ACRV1hrqA9Js",
    "outputId": "788fbbf7-0ca6-4e75-aea4-01ba63a0e3ab"
   },
   "outputs": [],
   "source": [
    "set_['sample'].float().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExWYesDp_Tqe"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3415411,
     "status": "ok",
     "timestamp": 1641889349634,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "STK2Fj9wgGrA",
    "outputId": "241d0be5-7e5c-425a-98e8-df35dace3bb6"
   },
   "outputs": [],
   "source": [
    "matrix_all = pd.DataFrame()\n",
    "start_year = 2011\n",
    "end_year = 2021 + 1 \n",
    "lag = 'all'\n",
    "\n",
    "\n",
    "for year_from in range(start_year, end_year, 1): \n",
    "  n = 0 \n",
    "  acc_all = []\n",
    "  feature_multy, x_train, y_train, x_test__, y_test__  = concat_of_split_1year(df_data_blue, year_from = year_from, axis = 2, lag = lag)\n",
    "  # input_shape = x_train.shape[1:]\n",
    "  x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.2, shuffle=True, random_state=42)\n",
    "  # print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "  train_dataset = Dataset_my(x_train, y_train)\n",
    "  test_dataset  = Dataset_my(x_test,  y_test)\n",
    "\n",
    "  train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                            batch_size=batch_size, \n",
    "                                            shuffle=False)\n",
    "\n",
    "  test_loader  = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                            batch_size=len(test_dataset), \n",
    "                                            shuffle=False)\n",
    "\n",
    "####################### Name dir\n",
    "  # type_date = 'no_last'\n",
    "  # if type_date == 'last':\n",
    "  #   type_dir = 'last_date/'\n",
    "  # else:\n",
    "  #   type_dir = 'list_date/'\n",
    "  \n",
    "  number_of_back = str(x_test.shape[1])  \n",
    "  checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks/ROBO/My_Feature_from_NET/pytorch/keras_from10_1830to60min/40/Gep/'\\\n",
    "                         + model_param + '_' + shape_set + '/'  + str(year_from)\n",
    "  print(checkpoint_filepath)\n",
    "  print(f'Test year: {year_from}')\n",
    "\n",
    "  # Train the model\n",
    "  n_total_steps = len(train_loader)\n",
    "  for epoch in range(num_epochs):\n",
    "      print(f'epoch: {epoch}')\n",
    "      for i, set_ in enumerate(train_loader): \n",
    "          # break \n",
    "          model.train()\n",
    "          outputs = model(set_['sample'].float())\n",
    "          loss = criterion(outputs, set_['target'])\n",
    "          \n",
    "          # Backward and optimize\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          \n",
    "          if (i+1) % 100 == 0:\n",
    "              print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "          # print('Wanna to see diffrences')\n",
    "          # print(model(torch.tensor(x_test__).float()).shape, model(torch.tensor(feature_multy[:,:,:]).float()))\n",
    "      # Test the model\n",
    "      # In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "      with torch.no_grad():\n",
    "          n_correct = 0\n",
    "          n_samples = 0\n",
    "          for i, set_ in enumerate(test_loader):\n",
    "              model.eval()\n",
    "              outputs = model(set_['sample'].float())\n",
    "              labels = set_['target']\n",
    "              # max returns (value ,index)\n",
    "              _, predicted = torch.max(outputs.data, 1)\n",
    "              n_samples += labels.size(0)\n",
    "              n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "          acc = 100.0 * n_correct / n_samples\n",
    "          acc_all.append(acc)\n",
    "          print(f'Accuracy of the network: {acc} %')\n",
    "          if len(acc_all) != 1:\n",
    "            if acc > max(acc_all[:-1]):     \n",
    "              print(f'Save model corse: {acc} more {acc_all} loss: {loss}')\n",
    "              # save\n",
    "              torch.save(model, checkpoint_filepath)\n",
    "          else:\n",
    "            print(f'Save model corse: {acc} more {acc_all} loss: {loss}')  \n",
    "            # save\n",
    "            torch.save(model, checkpoint_filepath)\n",
    "          \n",
    "  # matrix_year = df_data[(df_data['Date'].dt.year == year_from)].copy()\n",
    "  # matrix_year[proba_1] = model.predict(x_test)[:,-1]\n",
    "  # matrix_all = matrix_all.append(matrix_year)\n",
    "  # print(matrix_all.shape, matrix_year.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89Go47pAg8Ky"
   },
   "source": [
    "## Keras Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yh2rU5DT0qCo"
   },
   "source": [
    "### Build the model\n",
    "\n",
    "Our model processes a tensor of shape `(batch size, sequence length, features)`,\n",
    "where `sequence length` is the number of time steps and `features` is each input\n",
    "timeseries.\n",
    "\n",
    "You can replace your classification RNN layers with this one: the\n",
    "inputs are fully compatible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nNnCOrJ0qCq"
   },
   "source": [
    "We include residual connections, layer normalization, and dropout.\n",
    "The resulting layer can be stacked multiple times.\n",
    "\n",
    "The projection layers are implemented through `keras.layers.Conv1D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEC0r_Gz0qCq"
   },
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUuizYlj0qCr"
   },
   "source": [
    "The main part of our model is now complete. We can stack multiple of those\n",
    "`transformer_encoder` blocks and we can also proceed to add the final\n",
    "Multi-Layer Perceptron classification head. Apart from a stack of `Dense`\n",
    "layers, we need to reduce the output tensor of the `TransformerEncoder` part of\n",
    "our model down to a vector of features for each data point in the current\n",
    "batch. A common way to achieve this is to use a pooling layer. For\n",
    "this example, a `GlobalAveragePooling1D` layer is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2klDjg6t84X"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11901295,
     "status": "ok",
     "timestamp": 1642433752452,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "NlZ7Km1LuKu-",
    "outputId": "a29d5db0-e263-4641-8591-4147e1348408"
   },
   "outputs": [],
   "source": [
    "# надо ли перемешивать на тесте чтоы при батче 64 мы дивгали веса участь на всех бумажах а не на одной\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "matrix_all = pd.DataFrame()\n",
    "epochs = 100\n",
    "patience = 10\n",
    "start_year = 2015\n",
    "end_year = 2021 + 1 \n",
    "\n",
    "\n",
    "for year_from in range(start_year, end_year, 1):  \n",
    "  lag = 'all'\n",
    "  # feature_multy, x_train, y_train, x_test, y_test  = concat_of_split_1year(df_data, year_from = year_from, axis = 2, lag = lag)     # переход к reshape_concat_of_split_1year\n",
    "  feature_multy, x_train, y_train, x_test, y_test  = reshape_concat_of_split_1year(df_data, feature_name=feature_name, year_from = year_from)\n",
    "  input_shape = x_train.shape[1:]\n",
    "\n",
    "####################### Name dir\n",
    "  # type_date = 'no_last'\n",
    "  # if type_date == 'last':\n",
    "  #   type_dir = 'last_date/'\n",
    "  # else:\n",
    "  #   type_dir = 'list_date/'\n",
    "  \n",
    "  # number_of_back = str(x_test.shape[1])  \n",
    "  # dir_model = 'keras_30min1830/' + type_dir + 'gep1030_year_sicle/' + str(f'lag_{lag}') + '/blue/'    \n",
    "  # checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks/ROBO/My_Feature_from_NET/keras/' + dir_model + model_param + '_' + shape_set + '/' + str(year_from)\n",
    "  checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks/ROBO/My_Feature_from_NET/keras/keras_from10_1830to60min/40/Gep/'\\\n",
    "                         + model_param + '_' + shape_set + '/'  + str(year_from)\n",
    "  print(checkpoint_filepath)\n",
    "  print(f'Test year: {year_from}')\n",
    "########################\n",
    "\n",
    "  checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, \n",
    "                              monitor='val_accuracy',\n",
    "                              verbose=1, \n",
    "                              save_best_only=True,\n",
    "                              mode='auto')\n",
    "  EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience = patience, restore_best_weights=True)\n",
    "  model = build_model(\n",
    "      input_shape,\n",
    "      head_size=40,\n",
    "      num_heads=8,\n",
    "      ff_dim=4,\n",
    "      num_transformer_blocks=4,\n",
    "      mlp_units=[128],\n",
    "      mlp_dropout=0.4,\n",
    "      dropout=0.25,\n",
    "  )\n",
    "\n",
    "  model.compile(\n",
    "      loss=\"sparse_categorical_crossentropy\",\n",
    "      optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "      metrics=['sparse_categorical_accuracy', 'accuracy'],\n",
    "  )\n",
    "  # model.summary()\n",
    "\n",
    "  callbacks = [checkpoint, EarlyStopping]\n",
    "\n",
    "  model.fit(\n",
    "      x_train,\n",
    "      y_train,\n",
    "      validation_split=0.2,\n",
    "      epochs=epochs,\n",
    "      batch_size=64,\n",
    "      callbacks=callbacks)\n",
    "\n",
    "  model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "  matrix_year = df_data[(df_data['Date'].dt.year == year_from)].copy()\n",
    "  matrix_year[proba_1] = model.predict(x_test)[:,-1]\n",
    "  matrix_all = matrix_all.append(matrix_year)\n",
    "  print(matrix_all.shape, matrix_year.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOgOqpGstdv5"
   },
   "source": [
    "# Garbige\n",
    "# Drop_duplicate <----- df_date_conect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "py_3Pqq7Xt-P"
   },
   "outputs": [],
   "source": [
    "quant = 20\n",
    "\n",
    "defi = feature_name + [name_profit] + [name_profit_label] + [proba_1]\n",
    "new_date = df_data_conect.copy()\n",
    "new_date[proba_1 + '_quant_' + str(quant)] = pd.qcut(new_date[proba_1], quant, labels=False)\n",
    "pivo_quant = pd.pivot_table(new_date,\n",
    "               index = [proba_1 + '_quant_' + str(quant)],\n",
    "               values = [name_profit] + feature_name,\n",
    "               aggfunc = ['mean', 'count'],\n",
    "              #  margins = True\n",
    "               ).T.drop_duplicates().T\n",
    "pivo_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1640154852524,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "26HCPH7Tgpgb",
    "outputId": "de2ba70e-0f04-468e-cd7e-2b0145918410"
   },
   "outputs": [],
   "source": [
    "#LONG ONLY\n",
    "treshold = 0.6\n",
    "treshold_short = 0.45\n",
    "for_treshold = proba_ful\n",
    "\n",
    "df_data_conect.Date = pd.to_datetime(df_data_conect.Date)\n",
    "pivo_long = pd.pivot_table(df_data_conect[(df_data_conect[for_treshold].mean(axis = 1) > treshold)],\n",
    "               index = [df_data_conect.Date.dt.date],\n",
    "               values = [name_profit],\n",
    "               aggfunc = ['mean', 'count'],\n",
    "               margins = True\n",
    "               ).T.drop_duplicates().T\n",
    "display(pivo_long)\n",
    "pivo_long = pivo_long.iloc[:-1,:]\n",
    "pivo_long.reset_index(inplace = True)\n",
    "pivo_long.columns = ['Date', 'mean', 'count']\n",
    "pivo_long['Date'] = pd.to_datetime(pivo_long['Date'])\n",
    "pd.pivot_table(pivo_long,\n",
    "               index = pivo_long.Date.dt.year,\n",
    "               values = 'mean',\n",
    "               aggfunc = ['count', 'mean'],\n",
    "              margins = True\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "executionInfo": {
     "elapsed": 757,
     "status": "ok",
     "timestamp": 1639578863149,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "PMOqEKg0dBDM",
    "outputId": "67657011-3f54-4e8b-d0e3-32877e08f5a6"
   },
   "outputs": [],
   "source": [
    "# Long/Short == long1\n",
    "\n",
    "def only_long1(df_data_conect, name_profit, enter_long, enter_shrt, for_treshold, from_year, not_take):\n",
    "  # not_take = 'not_first'\n",
    "  # enter_long = 0.6\n",
    "  # enter_shrt = -0.4\n",
    "  # for_treshold = 'Min10_Min1000_Cl_max_day_Cl_min_day_4_17'\n",
    "\n",
    "  df_data_conect['Long_Short'] = np.where(df_data_conect[for_treshold] > enter_long, 1, np.where(df_data_conect[for_treshold] < enter_shrt, -1, -10))\n",
    "  df_data_conect['profit'] = df_data_conect[name_profit]*df_data_conect['Long_Short']\n",
    "  df_data_conect['proba_1_short_long'] = df_data_conect[for_treshold]*df_data_conect['Long_Short']\n",
    "\n",
    "  long_short_equity = df_data_conect.sort_values(['Date', 'Long_Short', 'proba_1_short_long'], ascending = [True, False, False], ignore_index=True)\n",
    "  long_short_equity['Long1'] = np.where(df_data_conect.Date.dt.date.shift(0) != df_data_conect.Date.dt.date.shift(1), 'first', 'not_first')\n",
    "  long_short_equity = long_short_equity[(long_short_equity['Long_Short'] != -10)&(long_short_equity['Long1'] != not_take)]\n",
    "\n",
    "\n",
    "  # df_data_conect.Date = pd.to_datetime(df_data_conect.Date)\n",
    "  pivo = pd.pivot_table(long_short_equity[long_short_equity.Date.dt.year >= from_year],\n",
    "                index = [long_short_equity.Date.dt.date],\n",
    "                values = ['profit'],\n",
    "                aggfunc = ['mean', 'count'],\n",
    "                margins = True\n",
    "                ).T.drop_duplicates().T\n",
    "  display(pivo)\n",
    "  pivo.reset_index(inplace = True)\n",
    "  pivo = pivo.iloc[:-1,:]\n",
    "  pivo.columns = ['Date', 'mean', 'count']\n",
    "  pivo['Date'] = pd.to_datetime(pivo['Date'])\n",
    "  display(pd.pivot_table(pivo,\n",
    "                index = pivo.Date.dt.year,\n",
    "                values = 'mean',\n",
    "                aggfunc = ['count', 'mean'],\n",
    "                margins = True\n",
    "                ))\n",
    "##########################################  \n",
    "not_take = 'not_first'\n",
    "enter_long = 0.6\n",
    "enter_shrt = -0.4\n",
    "for_treshold = proba_ful[0]\n",
    "from_year = 2011\n",
    "for_profit = 'profit_1hour'\n",
    "print(f'for_profit: {for_profit}')\n",
    "\n",
    "# df_data_conect['Long_Short'] = np.where(df_data_conect[proba_1] > enter_long, 1, np.where(df_data_conect[proba_1] < enter_shrt, -1, -10))\n",
    "# df_data_conect['profit'] = df_data_conect[name_profit]*df_data_conect['Long_Short']\n",
    "# df_data_conect['proba_1_short_long'] = df_data_conect[proba_1]*df_data_conect['Long_Short']\n",
    "\n",
    "# long_short_equity = df_data_conect.sort_values(['Date', 'Long_Short', 'proba_1_short_long'], ascending = [True, False, False], ignore_index=True)\n",
    "# long_short_equity['Long1'] = np.where(df_data_conect.Date.dt.date.shift(0) != df_data_conect.Date.dt.date.shift(1), 'first', 'not_first')\n",
    "\n",
    "only_long1(df_data_conect, for_profit, enter_long = enter_long, enter_shrt = enter_shrt, for_treshold = for_treshold, from_year = from_year, not_take = not_take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVMiajRq3WCX"
   },
   "outputs": [],
   "source": [
    "#  Предсказание на 30 минутках в ЦИКЛЕ\n",
    "# count\tmean\n",
    "# mean\tmean\n",
    "# Date\t\t\n",
    "# 2017\t75\t0.500150\n",
    "# 2018\t92\t0.405941\n",
    "# 2019\t42\t0.582666\n",
    "# 2020\t171\t0.633090\n",
    "# 2021\t140\t0.508307\n",
    "# All\t520\t0.536060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1638711488695,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "sAZfRzenNPpg",
    "outputId": "ef0a0904-689d-45e7-b063-7f3b9d51a94f"
   },
   "outputs": [],
   "source": [
    "# SHORT ONLY\n",
    "df_data_conect.Date = pd.to_datetime(df_data_conect.Date)\n",
    "pivo_short = pd.pivot_table(df_data_conect[(df_data_conect[proba_1] < 0.4)],\n",
    "               index = [df_data_conect.Date.dt.date],\n",
    "               values = [name_profit],\n",
    "               aggfunc = ['mean', 'count'],\n",
    "               margins = True\n",
    "               ).T.drop_duplicates().T\n",
    "display(pivo_short)\n",
    "pivo_short = pivo_short.iloc[:-1,:]\n",
    "pivo_short.reset_index(inplace = True)\n",
    "pivo_short.columns = ['Date', 'mean', 'count']\n",
    "pivo_short['Date'] = pd.to_datetime(pivo_short['Date'])\n",
    "pd.pivot_table(pivo_short,\n",
    "               index = pivo_short.Date.dt.year,\n",
    "               values = 'mean',\n",
    "               aggfunc = ['count', 'mean'],\n",
    "              margins = True\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sv9N_bk0KeJj"
   },
   "source": [
    "### Long and Short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "executionInfo": {
     "elapsed": 1245,
     "status": "ok",
     "timestamp": 1638516226982,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "YKBl3M6RKhBY",
    "outputId": "004cff17-fe18-4c5c-e0dd-782f1dd2328e"
   },
   "outputs": [],
   "source": [
    "enter_long = 0.6\n",
    "enter_shrt = 0.45\n",
    "not_take = 'not_first'\n",
    "\n",
    "df_data_conect['Long_Short'] = np.where(df_data_conect[proba_1] > enter_long, 1, np.where(df_data_conect[proba_1] < enter_shrt, -1, -10))\n",
    "df_data_conect['profit'] = df_data_conect['profit_GEP']*df_data_conect['Long_Short']\n",
    "df_data_conect['proba_1_short_long'] = df_data_conect[proba_1]*df_data_conect['Long_Short']\n",
    "\n",
    "long_short_equity = df_data_conect.sort_values(['Date', 'Long_Short', 'proba_1_short_long'], ascending = [True, False, False], ignore_index=True)\n",
    "long_short_equity['Long1'] = np.where(df_data_conect.Date.dt.date.shift(0) > df_data_conect.Date.dt.date.shift(1), 'first', 'not_first')\n",
    "long_short_equity = long_short_equity[(long_short_equity['Long_Short'] != -10)&(long_short_equity['Long1'] != not_take)]\n",
    "\n",
    "\n",
    "# df_data_conect.Date = pd.to_datetime(df_data_conect.Date)\n",
    "pivo = pd.pivot_table(long_short_equity,\n",
    "               index = [long_short_equity.Date.dt.date],\n",
    "               values = ['profit'],\n",
    "               aggfunc = ['mean', 'count'],\n",
    "               margins = True\n",
    "               ).T.drop_duplicates().T\n",
    "display(pivo)\n",
    "pivo.reset_index(inplace = True)\n",
    "pivo = pivo.iloc[:-1,:]\n",
    "pivo.columns = ['Date', 'mean', 'count']\n",
    "pivo['Date'] = pd.to_datetime(pivo['Date'])\n",
    "pd.pivot_table(pivo,\n",
    "               index = pivo.Date.dt.year,\n",
    "               values = 'mean',\n",
    "               aggfunc = ['count', 'mean'],\n",
    "              margins = True\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2qXyoq672cY"
   },
   "source": [
    "# Plot violone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 670
    },
    "executionInfo": {
     "elapsed": 1567,
     "status": "ok",
     "timestamp": 1637068937378,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "6_ZCw3VS75TZ",
    "outputId": "4c371b19-6836-45ae-97d5-ff37a8e35126"
   },
   "outputs": [],
   "source": [
    "# large = 22; med = 16; small = 12\n",
    "# params = {'axes.titlesize': large,\n",
    "#           'legend.fontsize': med,\n",
    "#           'figure.figsize': (4, 10),\n",
    "#           'axes.labelsize': med,\n",
    "#           'axes.titlesize': med,\n",
    "#           'xtick.labelsize': med,\n",
    "#           'ytick.labelsize': med,\n",
    "#           'figure.titlesize': large}\n",
    "# plt.rcParams.update(params)\n",
    "# plt.style.use('seaborn-whitegrid')\n",
    "# sns.set_style(\"white\")\n",
    "# %matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(20,10), dpi= 80)\n",
    "ax = sns.violinplot(x=new_date[proba_1 + '_quant_' + str(quant)], y = new_date[name_feature[1]],  scale='width', inner='quartile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbiSy7hvGWmX"
   },
   "source": [
    "# Оценка результата"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMSmCam6lVZS"
   },
   "source": [
    "## Склеиваем сделки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "error",
     "timestamp": 1637049380122,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "hwauSJLtlabR",
    "outputId": "6bf6d263-7f9a-4a25-8a4c-19ad787e9989"
   },
   "outputs": [],
   "source": [
    "df_f_all = matrix_year[matrix_year['Symbol'] != 'fGAZP'].copy()\n",
    "### СОРТИРОВАТЬ ПО SYMBOL!!!\n",
    "threshold_in = 0.53\n",
    "threshold_out = 0.50\n",
    "\n",
    "\n",
    "def mark(df_f_all, threshold_in = 0.52, threshold_out = 0.50, stop = False):\n",
    "  hour_np = np.array(df_f_all['Date'].dt.hour)\n",
    "  close_np = np.array(df_f_all['Close'])\n",
    "  proba2_range_np = np.array(df_f_all['proba2_range'])\n",
    "  min10_np = np.array(df_f_all['Min10'])\n",
    "  pr1000_np = np.array(df_f_all['Proft1000_norm'])\n",
    "  signal = np.zeros(shape=len(df_f_all))\n",
    "  close_hour = np.zeros(shape=len(df_f_all))\n",
    "  close_hour_1 = np.zeros(shape=len(df_f_all))\n",
    "  min_enter = np.zeros(shape=len(df_f_all))\n",
    "  pr1000_enter = np.zeros(shape=len(df_f_all))\n",
    "  profit_hour_np = np.zeros(shape=len(df_f_all))\n",
    "  profit_hour_np_1 = np.zeros(shape=len(df_f_all))\n",
    "  hold = np.zeros(shape=len(df_f_all))\n",
    "  hold_1 = np.zeros(shape=len(df_f_all))\n",
    "\n",
    "\n",
    "  for i in range(close_np.size):\n",
    "    # print(proba2_range_np[i], i)\n",
    "    # print(10*('ddddd'))\n",
    "    if i == 0:\n",
    "      # print(proba2_range_np[i])\n",
    "      signal[i] = 0\n",
    "    elif proba2_range_np[i] > threshold_in and (signal[i-1] == 0 or signal[i-1] == 3):\n",
    "      # print(proba2_range_np[i])\n",
    "      signal[i] = 1\n",
    "      hold[i] = 1\n",
    "    elif stop and hour_np[i] == 10 and (signal[i-1] == 2 or signal[i-1] == 1):      \n",
    "      signal[i] = 3\n",
    "      hold[i] = hold[i-1]\n",
    "    elif proba2_range_np[i] > threshold_out and (signal[i-1] == 2 or signal[i-1] == 1):\n",
    "      signal[i] = 2\n",
    "      hold[i] = 1 + hold[i-1]      \n",
    "    elif (proba2_range_np[i] <= threshold_out and (signal[i-1] == 2 or signal[i-1] == 1)):\n",
    "      signal[i] = 3\n",
    "      hold[i] = hold[i-1]\n",
    "    elif proba2_range_np[i] <= threshold_in and (signal[i-1] == 0 or signal[i-1] == 3):\n",
    "      # print(proba2_range_np[i])\n",
    "      signal[i] = 0\n",
    "      hold[i] = 0\n",
    "\n",
    "  df_f_all['signal'] = signal\n",
    "  df_f_all['hold'] = hold\n",
    "\n",
    "  for i in range(close_np.size):\n",
    "    # print(proba2_range_np[i], i)\n",
    "    # print(10*('ddddd'))\n",
    "    if i == 0 or signal[i] == 0:\n",
    "      close_hour[i] = None\n",
    "      min_enter[i] = None\n",
    "      pr1000_enter[i] = None\n",
    "      profit_hour_np[i] = (close_np[i]/close_hour[i] - 1)*100\n",
    "         \n",
    "    elif signal[i] == 1:\n",
    "      close_hour[i] = close_np[i]\n",
    "      min_enter[i]  = min10_np[i]\n",
    "      pr1000_enter[i]   = pr1000_np[i]\n",
    "      profit_hour_np[i] = None\n",
    "    else:\n",
    "      close_hour[i] = close_hour[i-1]\n",
    "      min_enter[i]  = min_enter[i-1]\n",
    "      pr1000_enter[i]   = pr1000_enter[i-1]\n",
    "      profit_hour_np[i] = (close_np[i]/close_hour[i] - 1)*100\n",
    "\n",
    "    # for i in range(close_np.size-1, -1, -1):\n",
    "    #   if signal[i] == 3 or i == close_np.size-1:\n",
    "    #     close_hour_1[i] = close_np[i]\n",
    "    #     profit_hour_np_1[i] = None\n",
    "    #   else:\n",
    "    #     close_hour_1[i] = close_hour_1[i+1]\n",
    "    #     profit_hour_np_1[i] = (close_hour_1[i]/close_np[i] - 1)*100\n",
    "\n",
    "  close_np_1 = close_np[::-1]\n",
    "  signal_1 = signal[::-1]\n",
    "  for i in range(close_np.size):\n",
    "    if signal_1[i] == 3 or i == 0:\n",
    "      close_hour_1[i] = close_np_1[i]\n",
    "      profit_hour_np_1[i] = None\n",
    "      hold_1[i] = 0\n",
    "    else:\n",
    "      close_hour_1[i] = close_hour_1[i-1]\n",
    "      profit_hour_np_1[i] = (close_hour_1[i]/close_np_1[i] - 1)*100\n",
    "      hold_1[i] = hold_1[i-1] + 1\n",
    "\n",
    "\n",
    "  df_f_all['close_hour'] = close_hour\n",
    "  df_f_all['profit_hour_np'] = profit_hour_np\n",
    "  df_f_all['profit_hour_np'] = np.where(df_f_all['profit_hour_np'] > 20, 20, np.where(df_f_all['profit_hour_np'] < -20, -20, df_f_all['profit_hour_np']))\n",
    "  df_f_all['min_enter'] = min_enter\n",
    "  df_f_all['pr1000_enter'] = pr1000_enter\n",
    "\n",
    "  \n",
    "  df_f_all['profit_hour_np_1'] = profit_hour_np_1[::-1]\n",
    "  df_f_all['profit_hour_np_1'] = np.where(df_f_all['profit_hour_np_1'] > 20, 20, np.where(df_f_all['profit_hour_np_1'] < -20, -20, df_f_all['profit_hour_np_1']))\n",
    "  df_f_all['close_hour_1'] = close_hour_1[::-1]\n",
    "  df_f_all['hold_1'] = hold_1[::-1]\n",
    "\n",
    "  return df_f_all\n",
    "\n",
    "df_f_all = mark(df_f_all, threshold_in = threshold_in, threshold_out = threshold_out, stop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2losjMZlaj7"
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(df_f_all[(df_f_all['signal'] == 1)&(df_f_all['Year'] > -2014)],\n",
    "               index = df_f_all.Date.dt.hour,\n",
    "               values = ['hold_1', 'Min1000', 'min_enter', 'Proft1000_norm', 'pr1000_enter', 'profit_hour_np_1', 'Proft1000_label'],\n",
    "               aggfunc=['count', 'mean'],\n",
    "               margins=True\n",
    "               ).T.drop_duplicates().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPtYZa1LlrcD"
   },
   "outputs": [],
   "source": [
    "# signal = 3\n",
    "pd.pivot_table(df_f_all[(df_f_all['signal'] == 3)&(df_f_all['Year'] > -2014)],\n",
    "               index = df_f_all.Date.dt.hour,\n",
    "               values = ['signal', 'profit_hour_np', 'hold', 'Min1000', 'min_enter', 'Proft1000_norm', 'pr1000_enter', 'proba2_range'],\n",
    "               aggfunc=['count', 'mean'],\n",
    "               margins=True\n",
    "               ).T.drop_duplicates().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4aL0jFBljJ_"
   },
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1846,
     "status": "ok",
     "timestamp": 1638516260981,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "IVKISQFrd2Um",
    "outputId": "3eaf1c1c-d0a5-4dd8-e34f-9cc01b90c789"
   },
   "outputs": [],
   "source": [
    "from_year = 2010\n",
    "comission = 0.1/100\n",
    "name_for_p = ['pivo_long', 'pivo_short', 'pivo']\n",
    "equity_dic = dict.fromkeys(name_for_p)\n",
    "\n",
    "\n",
    "for p, name_p in zip ([pivo_long, pivo_short, pivo], name_for_p):\n",
    "  print(f'Сделки: {name_p}')\n",
    "  pivo_for_plot = p[p.Date.dt.year >= from_year]  \n",
    "  equity = np.zeros(shape = len(pivo_for_plot.iloc[:-1,:]))\n",
    "  equity_reinvest = np.zeros(shape = len(pivo_for_plot.iloc[:-1,:]))\n",
    "  if name_p != 'pivo_short':\n",
    "    profit_hour_np_1 = np.array(pivo_for_plot[('mean')])[:-1]\n",
    "  else:\n",
    "    profit_hour_np_1 = np.array(-pivo_for_plot[('mean')])[:-1]\n",
    "\n",
    "  for i in range(len(equity)):\n",
    "    if i == 0:\n",
    "      equity[i] = 100 + 100*(profit_hour_np_1[i]/100 - comission)\n",
    "      equity_reinvest[i] = 100*(1 + profit_hour_np_1[i]/100 - comission)\n",
    "    else:\n",
    "      equity[i] = equity[i-1] + 100*(profit_hour_np_1[i]/100 - comission)\n",
    "      equity_reinvest[i] = equity_reinvest[i-1]*(1 + profit_hour_np_1[i]/100 - comission)\n",
    "\n",
    "  # скачали индексы\n",
    "  list_index = dict.fromkeys(['SANDP', 'IMOEX', 'MOEX10', 'RTS'])\n",
    "  # for i, keys in zip(os.listdir(DIR + 'Data/all/index/'), ['SANDP', 'IMOEX', 'MOEX10', 'RTS']):\n",
    "  for i, keys in zip(['IMOEX.csv'], ['IMOEX']):\n",
    "    list_index[keys] = pd.read_csv(DIR + 'Data/all/index/' + i, parse_dates=['<DATE>'])\n",
    "    list_index[keys]['Year'] = list_index[keys]['<DATE>'].dt.year\n",
    "\n",
    "  # построили df_equity\n",
    "  df_equity = pd.DataFrame([np.array(pivo_for_plot['Date']), equity[:]]).T\n",
    "  df_equity.columns = ['<DATE>', 'equity']\n",
    "  df_equity['<DATE>'] = pd.to_datetime(df_equity['<DATE>']).dt.date\n",
    "  df_equity['<DATE>'] = pd.to_datetime(df_equity['<DATE>'])\n",
    "\n",
    "  # обьеденили \n",
    "  for_plot = pd.merge(list_index['IMOEX'][list_index['IMOEX']['Year']>=from_year], df_equity, on = '<DATE>', how = 'left')\n",
    "  # for_plot = pd.merge(for_plot, list_index['MOEX10'], on = '<DATE>', how = 'left', suffixes=('', 'MOEX10'))\n",
    "  for_plot['equity'].fillna(method='ffill', inplace=True)\n",
    "  equity_dic[name_p] = list(for_plot['equity'])\n",
    "  \n",
    "\n",
    "  # Plot\n",
    "  plt.figure(figsize=(20, 10))\n",
    "  plt.plot(for_plot['<DATE>'], for_plot['equity'], label='my_equity')\n",
    "  # plt.plot(for_plot['<DATE>'], equity_max, label='my_equity')\n",
    "  plt.plot(for_plot['<DATE>'], for_plot['<CLOSE>']*100/for_plot['<CLOSE>'][0], label='IMOEX')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjIVNoPYGyfU"
   },
   "source": [
    "## Метрики WL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3826,
     "status": "ok",
     "timestamp": 1638516284323,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "PK_2kBL1bypD",
    "outputId": "92772599-b28c-4329-d2a7-0f9678fe38b5"
   },
   "outputs": [],
   "source": [
    "# name_for_p = ['pivo_long', 'pivo_short', 'pivo']\n",
    "equity_analize = 'equity'\n",
    "for eq in name_for_p:\n",
    "  print(eq)\n",
    "  # equity_from_plot = for_plot[equity_analize]\n",
    "  equity_from_plot = pd.Series(equity_dic[eq])\n",
    "  equity_max = np.zeros(shape = len(equity_from_plot))\n",
    "\n",
    "  for i in range(len(equity_max)):\n",
    "    if i == 0:\n",
    "      equity_max[i] = equity_from_plot[i]\n",
    "    else:\n",
    "      equity_max[i] = (equity_from_plot[:i+1]).max()\n",
    "\n",
    "  for_plot['equity_max'] = equity_max\n",
    "\n",
    "  ddown_bar = []\n",
    "  for i in range(len(for_plot['equity'])):\n",
    "    if i == 0:\n",
    "      k = 0\n",
    "    elif equity_max[i] == equity_max[i - 1]:\n",
    "      k += 1\n",
    "    else:\n",
    "      k = 1\n",
    "    ddown_bar.append(k)\n",
    "    # print(k)\n",
    "\n",
    "  for_plot['DrowDown_bar'] = ddown_bar\n",
    "\n",
    "  for_plot['DrowDown'] = (for_plot['equity_max'] / equity_from_plot - 1) * 100\n",
    "\n",
    "  # Количество дней до нового максимума\n",
    "  plt.figure(figsize=(20, 10))\n",
    "  plt.plot(for_plot['<DATE>'], for_plot['DrowDown_bar'])\n",
    "  plt.show()\n",
    "\n",
    "  # Просадка счета от последнего хая\n",
    "  plt.figure(figsize=(20, 10))\n",
    "  plt.plot(for_plot['<DATE>'], for_plot['DrowDown'])\n",
    "  plt.show()\n",
    "\n",
    "  # % Profit Distribution of Trades\n",
    "\n",
    "  plt.figure(figsize=(20, 10))\n",
    "  plt.hist(pivo[('mean')], bins = 100)\n",
    "  plt.show()\n",
    "  # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1635961749033,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "GQBRJKRNG8iA",
    "outputId": "33435db4-d3cf-446a-9c02-e8027b4da8fb"
   },
   "outputs": [],
   "source": [
    "# Win_rate\n",
    "win_rate = np.where(pivo[('mean')] >= 0, 1, 0)\n",
    "win_rate = round(win_rate.mean()*100, 2)\n",
    "loss_rate = round(100 - win_rate, 2)\n",
    "print('Win_rate')\n",
    "display(win_rate, loss_rate)\n",
    "\n",
    "# Прибыль на 1 сделку:\n",
    "Profit_trade = round(pivo[('mean')].mean(), 2)\n",
    "print('Прибыль на 1 сделку')\n",
    "display(Profit_trade)\n",
    "\n",
    "# Средняя прибыль в процентах на прибыльную и убыток на убыточную сделку\n",
    "Average_profit = round(pivo[pivo[('mean')]  > 0][[('mean')]].mean(), 2)\n",
    "Average_loss   = round(pivo[pivo[('mean')] <= 0][[('mean')]].mean(), 2)\n",
    "print('Profit_trade, Average_profit, Average_loss')\n",
    "display(Profit_trade, Average_profit, Average_loss)\n",
    "\n",
    "# Профит фактор\n",
    "profit_factor = (pivo[pivo[('mean')] > 0][[('mean')]].sum()/-pivo[pivo[('mean')] <= 0][[('mean')]].sum()).round(2)\n",
    "profit_factor\n",
    "print('profit_factor')\n",
    "display(profit_factor)\n",
    "\n",
    "\n",
    "# Насколько прибыль превышает глубину  максимальной просадки\n",
    "Recovery_factor = round((for_plot['equity'][-1:] / 100 - 1) * 100 / (for_plot['DrowDown']).max(), 2)\n",
    "Recovery_factor\n",
    "print('Recovery_factor')\n",
    "display(Recovery_factor)\n",
    "\n",
    "# Cоотношение средней прибыльной сделки к средней убыточной\n",
    "Payoff_ratio = round(Average_profit/-Average_loss, 2)\n",
    "Payoff_ratio\n",
    "print('Payoff_ratio')\n",
    "display(Payoff_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIDiZF4u6z9-"
   },
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1265,
     "status": "ok",
     "timestamp": 1637085681412,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "b6GvFitD612d",
    "outputId": "ff50e4bd-3418-409e-8279-528f364f3930"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [11, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7ylQZrt9ikV"
   },
   "source": [
    "## KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7D2KuCc7NA5"
   },
   "outputs": [],
   "source": [
    "all_1 = df_data_conect[df_data_conect[model_param] >= 0.6].copy()\n",
    "X = all_1[name_feature]\n",
    "X_all = df_data_conect[name_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKllI6wLAbu2"
   },
   "outputs": [],
   "source": [
    "k_means = KMeans(n_clusters = 5)\n",
    "k_means = k_means.fit(X)\n",
    "clusters = k_means.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "executionInfo": {
     "elapsed": 422,
     "status": "ok",
     "timestamp": 1637085731990,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "D6DgsaDZ7AYz",
    "outputId": "42e1ebba-723d-451e-9db5-188ef5bdd037"
   },
   "outputs": [],
   "source": [
    "all_1['clusters'] = clusters\n",
    "\n",
    "display(pd.pivot_table(all_1[all_1['clusters'] != -2],\n",
    "               index = ['test_train' ,'clusters'],\n",
    "               values = [name_profit, name_profit_label] + ['Min10_' + str(i) for i in range(10,19)],\n",
    "               aggfunc = ['count', 'mean'],\n",
    "              #  margins = True\n",
    "               ).T.drop_duplicates().T.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIWDZVfE9XTh"
   },
   "source": [
    "## TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 73774,
     "status": "ok",
     "timestamp": 1637085840969,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "-rlwyHMm9bXd",
    "outputId": "7bae9478-2ac5-4b81-9373-6a493522af7e"
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "new_codes = tsne.fit_transform(X)\n",
    "\n",
    "all_1['tsne_x'] = new_codes[:,0]\n",
    "all_1['tsne_y'] = new_codes[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "executionInfo": {
     "elapsed": 798,
     "status": "ok",
     "timestamp": 1637085841696,
     "user": {
      "displayName": "Marat Kh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12243561143564302147"
     },
     "user_tz": -180
    },
    "id": "WZRuw8pJ7AdL",
    "outputId": "34095b4e-d4fe-470b-88a7-fa980804c71b"
   },
   "outputs": [],
   "source": [
    "for cluster, color in zip(all_1['clusters'].unique(), ['r', 'g', 'blue', 'yellow', 'black']):\n",
    "  plt.scatter(all_1[all_1['clusters'] == cluster]['tsne_x'], all_1[all_1['clusters'] == cluster]['tsne_y'], c = color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkGM8PdyjebE"
   },
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JQBP53VXHcK"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=10, max_depth=20)\n",
    "rfc.fit(x_train.squeeze(), y_train, )\n",
    "\n",
    "rt = DecisionTreeClassifier(max_depth=4)\n",
    "rt.fit(x_train.squeeze(), y_train, )\n",
    "\n",
    "\n",
    "ful_date = new_date[[str(i) for i in list(range(10,19))]].squeeze()\n",
    "xx_predRFC = rfc.predict_proba(ful_date)\n",
    "xx_predRT = rt.predict_proba(ful_date)\n",
    "\n",
    "metkaRFC = xx_predRFC\n",
    "metkaRT = xx_predRT\n",
    "# label = all['Min10_label']\n",
    "label = new_date['profit_GEP_label']\n",
    "# label = y_test\n",
    "\n",
    "treshold = 0.5\n",
    "tresholdUp = 10.57\n",
    "# metkaRFC = np.where(metkaRFC[:,1] > 0.5, 1, 0)\n",
    "# metkaRT = np.where(metkaRT[:,1] > 0.5, 1, 0)\n",
    "\n",
    "metkaRFC = np.where((metkaRFC[:,1] > treshold) & (metkaRFC[:,1] < tresholdUp), 1, 0)\n",
    "metkaRT = np.where(metkaRT[:,1] > treshold, 1, 0)\n",
    "\n",
    "\n",
    "display(confusion_matrix(label, metkaRFC), confusion_matrix(label, metkaRT))\n",
    "\n",
    "\n",
    "new_date['foreRFC'] = metkaRFC\n",
    "display(pd.pivot_table(new_date,\n",
    "               index = ['test_train' ,'foreRFC'],\n",
    "               values = ['profit_GEP', 'profit_GEP_label'] + [str(i) for i in list(range(10,19))],\n",
    "               aggfunc = ['count', 'mean']\n",
    "              #  margins = True\n",
    "               ).T.drop_duplicates().T.head(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6sG8YUaobZb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "k-etkfh_5JeP",
    "isUWvqGY_HRe",
    "iaCC_1jY5XqT",
    "Z5aquefQ0qQi",
    "s2ZmHqhG-xbU",
    "PRgnFo2xxTay",
    "qiG3NLvAhIsn",
    "89Go47pAg8Ky",
    "TOgOqpGstdv5",
    "_2qXyoq672cY",
    "DbiSy7hvGWmX",
    "gMSmCam6lVZS",
    "wIDiZF4u6z9-",
    "CkGM8PdyjebE"
   ],
   "name": "Copy of Keras_transformer_classification",
   "provenance": [
    {
     "file_id": "1EARRzQ-5-luHwaQOAUKIrCluYl3iBNfw",
     "timestamp": 1651337911690
    },
    {
     "file_id": "1l_hDKNG38Xn2PmPR_pY_I0Jyqml_h8JM",
     "timestamp": 1635923992732
    },
    {
     "file_id": "https://github.com/keras-team/keras-io/blob/master/examples/timeseries/ipynb/timeseries_classification_transformer.ipynb",
     "timestamp": 1635532950222
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
